---
title: "ML Algorithms for classification"
subtitle: "Introduction to neural networks"
venue: "ITViate data science courses"
type: "invited"
recording: "none"
output:
  ioslides_presentation:
    transition: default
    widescreen: true
    standalone: false
css: styles.css
date: "Slides -- <http://bit.ly/2q9NPSU>  <br /> <br />"
author: "Hicham Zmarrou"
---


```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  fig.width = 10,
  fig.height = 4,
  comment = "#>",
  collapse = TRUE,
  warning = FALSE
)
```

        
## Aims of this lesson 

+ Give you a general introduction to neural networks, How they works and how to implement them in  `R`. 



## What are Neural networks

https://www.youtube.com/watch?v=bxe2T-V8XRs

https://www.youtube.com/watch?v=UJwK6jAStmg

https://www.youtube.com/watch?v=5u0jaA3qAGk&t=45s

https://www.youtube.com/watch?v=GlcnxUlrtek

https://www.youtube.com/watch?v=pHMzNW8Agq4

https://www.youtube.com/watch?v=9KM9Td6RVgQ

https://www.youtube.com/watch?v=S4ZUwgesjS8



## Neural network from scratch

```{r, echo=FALSE}
library(ggplot2)
moons <- read.csv("./data/moonsXY.csv")
#moons[,1] <- NULL 
sp<-ggplot(moons, aes(x=xcoord, y=ycoord, color=as.factor(label))) + geom_point()
sp
```

+ The dataset we generated has two classes, plotted as red and blue points. 

+ You can think of the blue dots as male patients and the red dots as female patients, with the x- and y- axis being medical measurements.

## Neural network from scratch

+ Our goal is to train a Machine Learning classifier that predicts the correct class (male of female) given the x- and y- coordinates.

+ Data is not linearly separable, we can't draw a straight line that separates the two classes. 

+ This means that linear classifiers, such as Logistic Regression, won't be able to fit the data unless you hand-engineer non-linear features (such as polynomials) that work well for the given dataset.


+ That's one of the major advantages of Neural Networks. You don't need to worry about feature engineering. The hidden layer of a neural network will learn features for you.

## Fitting the logistic regression 

```{r, echo=FALSE}
source("myplot.R")
model <- glm(label~., data = moons, family=binomial(link='logit'))
## Warning: glm.fit: algorithm did not converge
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
class(model) <- c("lr", class(model))
predict.lr <- function(object, newdata, ...)
  predict.glm(object, newdata, type = "response") > .5

myplot(model, moons, class = "label", main = "Logistic Regression")

```

+ The graph shows the decision boundary learned by our Logistic Regression classifier. 

+ It separates the data as good as it can using a straight line, but it's unable to capture the "moon shape" of our data.

## Training a neural network 

*  Build a 3-layer neural network with one input layer,

    + one input layer, one hidden layer, and one output layer. 
    
    + the number of nodes in the input layer is determined by the dimensionality of our data, 2. 
    
    + the number of nodes in the output layer is determined by the number of classes we have, also 2.
    
    + the input to the network will be xcoord- and ycoord and its output will be two probabilities, one for class red ("female") and one for class blue ("male"). It looks something like this:
    
## Training a neural network 

<div align="center">
  <img src="img/nn-from-scratch-3-layer-network.png"  width="70%" height="70%"/> 
</div>

## Training a neural network 

We also need to pick an activation function for our hidden layer.  activation function transforms the inputs of the layer into its outputs.

* nonlinear activation function is what allows us to fit nonlinear hypotheses. 

* common chocies for activation functions are [tanh](https://reference.wolfram.com/language/ref/Tanh.html), the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function, or [ReLUs](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))

* Because we want our network to output probabilities the activation function for the output layer will be the [softmax](https://en.wikipedia.org/wiki/Softmax_function) 

## How our network makes predictions

* Our network makes predictions using forward propagation (bunch of matrix multiplications and the application of the activation functions

$$
z_1 = x.W_1 + b_1\\ 
a_1 = \tanh(z_1)\\
z_2 = a_1.W_2 + b_2\\
a_2 = \hat{y} = \text{softmax}(z_2)
$$

* $z_i$ is the input of layer $i$ and $a_i$ is the output of layer $i$ after applying the activation function. 

* $W_1, b_1, W_2, b_2$ are parameters of our network, which we need to learn from our training data. 

If we use 500 nodes for the hidden layer then $W_1 \in \mathbb{R}^{2\times500}$, $b_1 \in \mathbb{R}^{500}$, $W_2 \in \mathbb{R}^{500\times2}$ and $b_2 \in \mathbb{R}^{2}$ 

## Learning the parameters

* Learning the parameters for our network means finding parameters $(W_1, b_1, W_2, b_2)$ that minimize the error on our training data
 
* We call the function that measures our error the loss function. 

* A common choice with the softmax output is the categorical [cross-entropy loss](http://bit.ly/2pm9KbO) (also known as negative log likelihood)

* If we have $N$ training examples and $C$ classes then the loss for our prediction $\hat{y}$ with respect to the true labels $y$ is given by:

$$L(y,\hat{y}) = -\frac{1}{N }\sum_{n\in N}\Big(  y_{n}\log\hat{y_{n}}  + (1-y_{n}) \log(1-\hat{y_n})\Big)$$ 

* The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class.

* We can use gradient descent to find the minimum and I will implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. 

## Learning the parameters

* Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice. So if you are serious you'll want to use one of these.

* As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameter   $\frac{\partial{L}}{\partial{W_1}}$, $\frac{\partial{L}}{\partial{b_1}}$, $\frac{\partial{L}}{\partial{W_2}}$, $\frac{\partial{L}}{\partial{b_2}}$. 

* To calculate these gradients we use the famous backpropagation algorithm, which is a way to efficiently calculate the gradients starting from the output.

I won't go into detail how backpropagation works, but there are many excellent explanations ([here](http://colah.github.io/posts/2015-08-Backprop/) or [here]()) floating around the web.

## Learning the parameters

Applying the backpropagation formula we find the following (trust me on this):


$\delta_3 = \hat{y} - y$   

$\delta_2 = (1 - \tanh^2 z_1) \circ \delta_3W_2^T$

$\frac{\partial{L}}{\partial{W_2}} = a_1^T \delta_3$ 

$\frac{\partial{L}}{\partial{b_2}} = \delta_3$

$\frac{\partial{L}}{\partial{W_1}} = x^T \delta2$ 

$\frac{\partial{L}}{\partial{b_1}} = \delta2$


## Implementation 

We start by defining some useful variables and parameters for gradient descent:

```{r}
num_examples = nrow(moons) 

nn_input_dim = 2 # input layer dimensionality
nn_output_dim = 2 # output layer dimensionality
 # Gradient descent parameters (I picked these by hand)

epsilon = 0.01 # learning rate for gradient descent
reg_lambda = 0.01 # regularization strength

```

First let's implement the loss function we defined above. We use this to evaluate how well our model is doing:




```{r}
##################################################################################
# Project:      Neural networks from scratch
# Description:  Calculate Loss function
# Data:         moons data set
# By:           Hicham Zmarrou 
# url:          www.trefoil.ml
##################################################################################

# Helper function to evaluate the total loss on the dataset
# Helper function to evaluate the total loss on the dataset


calculate_nn_loss <- function(model){
  
  # load the model parameters
  #X  = X ; y =y
  W1 = model$W1    
  b1 = model$b1
  W2 = model$W2
  b2 = model$b2
  # Forward propagation to calculate our predictions
  z1         = X%*%W1 + b1
  a1         = tanh(z1)
  z2         = a1%*%W2 + b2
  exp_scores = exp(z2)
  probs      = exp_scores/rowSums(exp_scores)
  probsXum   = rowSums(probs * cbind( (1-y),y)) 
  # Calculating the loss
  
  corect_logprobs = -log(probsXum)
  data_loss       =  sum(corect_logprobs)
  
  # Add regulatization term to loss (optional)
  
  data_loss  =   data_loss + reg_lambda/2 * (sum(W1^2) + sum(W2^2))
  return (1/ num_examples * data_loss)
}
  
```



We also implement a helper function to calculate the output of the network. It does forward propagation as defined above and returns the class with the highest probability.


```{r}
##################################################################################
# Project:      Neural networks from scratch
# Description:  Helper function to predict an output (0 or 1)
# Data:         moons data set
# By:           Hicham Zmarrou 
# url:          www.trefoil.ml
##################################################################################

# We also implement a helper function to calculate the output of the network. 
# It does forward propagation as defined above and returns the class with the highest probability.


predict_nn <- function(model, X){

  # load the model parameters
  
  W1 = model$W1    
  b1 = model$b1
  W2 = model$W2
  b2 = model$b2

  # Forward propagation to calculate our predictions
  z1         = X%*%W1 + b1
  a1         = tanh(z1)
  z2         = a1%*%W2 + b2
  exp_scores = exp(z2)
  
  probs      = exp_scores/rowSums(exp_scores)
  apply(probs, 1, which.max) - 1 
  return (apply(probs, 1, which.max) - 1)
}
  
  
  
```



Finally, here comes the function to train our Neural Network. It implements batch gradient descent using the backpropagation derivates we found above.

```{r}
# This function learns parameters for the neural network and returns the model.
# - nn_hdim: Number of nodes in the hidden layer
# - num_passes: Number of passes through the training data for gradient descent
# - print_loss: If True, print the loss every 1000 iterations
build_nn_model <- function(nn_hdim = 3,num_passes=20000,X,Y,print_loss = F){
 
  nn_hdim = 3 ; print_loss = TRUE 
  X = as.matrix(moons[,1:2])
  y = as.matrix(moons[,3])
  set.seed(1)
  
  W1 <- read_delim("./data/W1.txt","\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
  W1 <- as.matrix(W1)
  #W1  = matrix(rnorm(nn_input_dim*nn_hdim),nn_input_dim, nn_hdim)/sqrt(nn_input_dim)
  b1  = numeric(nn_hdim)
  
  ###########
  W2 <- read_delim("./data/W2.txt", "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
  W2 <- as.matrix(W2)
  #W2  = matrix(rnorm(nn_hdim*nn_output_dim),nn_hdim,nn_input_dim)/sqrt(nn_hdim)
  b2  = numeric(nn_output_dim)
  
  num_passes = 20 
    
  model <- list() # This is what we return at the end
  
  for (i in 1:num_passes){
    
    z1 =  X%*%W1 + b1
    a1 =  tanh(z1)
    z2 =  a1%*%W2 + b2
    exp_scores = exp(z2)
    probs = (exp_scores / rowSums(exp_scores))
    
    # Backpropagation
    delta3 =  probs
    delta3 =  delta3 - matrix(c(1-y,y),200,2)
    # delta3[range(num_examples), y] -= 1
    dW2 = t(a1) %*% delta3 
    #dW2 = (a1.T).dot(delta3)
    db2 = colSums(delta3)
    
    delta2 = delta3 %*% t(W2) * (1 - (a1^2))
    dW1 = t(X) %*%  delta2
    db1 = colSums(delta2)
    
    # Add regularization terms (b1 and b2 don't have regularization terms)
    dW2 <-  dW2 + reg_lambda*W2
    dW1 <-  dW1 + reg_lambda*W1
    
    # Gradient descent parameter update
    W1 <- W1 - epsilon * dW1
    b1 <- b1 - epsilon * db1
    W2 <- W2 - epsilon * dW2
    b2 <- b2 - epsilon * db2
    
    model$W1 =  W1
    model$b1 =  b1
    model$W2 =  W2
    model$b2 =  b2
    
    # Optionally print the loss.
    # # This is expensive because it uses the whole dataset, so we don't want to do it too often.
    
    if ((print_loss == TRUE)  & (i %% 1000 == 0)) {
      print(paste("Loss after iteration", i , ": " , calculate_nn_loss(model)))
    }

  }
  # Optionally print the loss.
  # # This is expensive because it uses the whole dataset, so we don't want to do it too often.
  return(model)             
} 

```

## A network with a hidden layer of size 3



```{r}
x <-  moons[1:150, c("xcoord", "ycoord", "label")]
x$label <- as.factor(x$label)
levels(x$label) <- c("m","f") 
head(x)
library(nnet)
model <- nnet(label ~ ., data=x, size =3, maxit = 1000, trace = FALSE)


```

## A network with a hidden layer of size 3


```{r}
myplot(model, x, class = "label", main = "NN (3)")
```

## Circle dataset

```{r}
set.seed(1000)

library(mlbench)
x <- mlbench.circle(100)
#x <- mlbench.cassini(100)
#x <- mlbench.spirals(100, sd = .1)
#x <- mlbench.smiley(100)
x <- cbind(as.data.frame(x$x), factor(x$classes))
colnames(x) <- c("x", "y", "class")

head(x)
```

## Circle dataset (Logistic Regression)

Logistic Regression
Only considers for 2 classes

```{r}
model <- glm(class ~., data = x, family=binomial(link='logit'))
class(model) <- c("lr", class(model))
predict.lr <- function(object, newdata, ...)
  predict.glm(object, newdata, type = "response") > .5

myplot(model, x, class = "class", main = "Logistic Regression")
```

## Circle dataset (Decision trees)

```{r}

library("rpart")
model <- rpart(class ~ ., data=x)
myplot(model, x, class = "class", main = "CART")
```


## Circle dataset (Decision trees overfitting)

```{r}
model <- rpart(class ~ ., data=x,
  control = rpart.control(cp = 0.001, minsplit = 1))
myplot(model, x, class = "class", main = "CART (overfitting)")

```

## Circle dataset (Decision trees C5.0)

```{r}
library(C50)
model <- C5.0(class ~ ., data=x)
myplot(model, x, class = "class", main = "C5.0")
```


## Circle dataset (Random Forest)

```{r}
library(randomForest)
model <- randomForest(class ~ ., data=x)
myplot(model, x, class = "class", main = "Random Forest")
```


## Circle dataset (Neural Network)

```{r}
library(nnet)
model <- nnet(class ~ ., data=x, size = 1, maxit = 1000, trace = FALSE)
myplot(model, x, class = "class", main = "NN (1)")

```


## Circle dataset (Neural Network)

```{r}
library(nnet)
model <- nnet(class ~ ., data=x, size = 2, maxit = 1000, trace = FALSE)
myplot(model, x, class = "class", main = "NN (2)")

```


## Circle dataset (Neural Network)


```{r}
library(nnet)
model <- nnet(class ~ ., data=x, size = 4, maxit = 1000, trace = FALSE)
myplot(model, x, class = "class", main = "NN (4)")

```

## Circle dataset (Neural Network)

```{r}
library(nnet)
model <- nnet(class ~ ., data=x, size = 10, maxit = 1000, trace = FALSE)
myplot(model, x, class = "class", main = "NN (10)")

```


