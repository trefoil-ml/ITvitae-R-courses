---
title: "Databases"
output: html_notebook
---


As well as working with local in-memory data like data frames and data tables, dplyr also works with remote on-disk data stored in databases. Generally, if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more hassle. The reason you’d want to use dplyr with a database is because either your data is already in a database (and you don’t want to work with static csv files that someone else has dumped out for you), or you have so much data that it does not fit in memory and you have to use a database. Currently dplyr supports the three most popular open source databases (sqlite, mysql and postgresql), and google’s bigquery.

Since R almost exclusively works with in-memory data, if you do have a lot of data in a database, you can’t just dump it into R. Instead, you’ll have to work with subsets or aggregates. dplyr aims to make this task as easy as possible. If you’re working with large data, it’s also likely that you’ll need support to get the data into the database and to ensure you have the right indices for good performance. While dplyr provides some simple tools to help with these tasks, they are no substitute for a local expert.

The motivation for supporting databases in dplyr is that you never pull down the right subset or aggregate from the database on your first try. Usually you have to iterate between R and SQL many times before you get the perfect dataset. But because switching between languages is cognitively challenging (especially because R and SQL are so perilously similar), dplyr helps you by allowing you to write R code that is automatically translated to SQL. The goal of dplyr is not to replace every SQL function with an R function; that would be difficult and error prone. Instead, dplyr only generates SELECT statements, the SQL you write most often as an analyst.

To get the most out of this tutorial, you’ll need to be familiar with querying SQL databases using the SELECT statement. If you have some familiarity with SQL and you’d like to learn more, I found [how indexes work in SQLite](http://www.sqlite.org/queryplanner.html) and [10 easy steps to a complete understanding of SQL](https://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql/) to be particularly helpful.

Getting started

The easiest way to experiement with databases using dplyr is to use SQLite. This is because everything you need is already included in the R package. You won’t need to install anything, and you won’t need to deal with the hassle of setting up a database server. Doing so is really easy: just give the path and the ok to create a table.

The easiest way to experiement with databases using dplyr is to use SQLite. This is because everything you need is already included in the R package. You won’t need to install anything, and you won’t need to deal with the hassle of setting up a database server. Doing so is really easy: just give the path and the ok to create a table.

```{r }

library(dplyr)
library(DBI)
my_db <- src_sqlite("my_db.sqlite3", create = T)

```

The main new concept here is the `src`, which is a collection of types of database tables. Use `src_sqlite()`, `src_mysql()`, `src_postgres()` and `src_bigquery()` to connect to the specific types supported by `dplyr`.

`my_db` currently has no data in it, so we’ll import the flights data using the convenient ``copy_to()`` function. This is a quick and dirty way of getting data into a database. Because all the data has to flow through `R`, you should note that this is not suitable for very large datasets.

```{r}
library(nycflights13)
flights_sqlite <- copy_to(my_db, flights, temporary = FALSE, indexes = list(
  c("year", "month", "day"), "carrier", "tailnum"))
```

As you can see, the `copy_to()` operation has an additional argument that allows you to supply indexes for the table. Here we set up indexes that will allow us to quickly process the data by day, by carrier and by plane. `copy_to()` also executes the SQL ANALYZE command: this ensures that the database has up-to-date table statistics and performs the appropriate query optimisations.

For this particular dataset, there’s a built-in src that will cache flights in a standard location:

```{r}
flights_sqlite <- tbl(nycflights13_sqlite(), "flights")
```




